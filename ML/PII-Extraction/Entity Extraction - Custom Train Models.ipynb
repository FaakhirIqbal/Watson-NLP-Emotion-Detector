{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Custom Train Models for Entity Etraction using Watson NLP"]},{"cell_type":"markdown","metadata":{},"source":["## Use Case\n","\n","\n","This notebook demonstrates how to train entity extraction models using Watson NLP. The goal of entity extraction is to automatically identify and classify specific entities or concepts within a text, such as people, organizations, locations, dates, times, and more.\n","\n","\n","## What you'll learn in this notebook\n","\n","Watson NLP implements state-of-the-art classification algorithms from three different families: \n","- Classic machine learning using CRF (Conditional Random Field)\n","- Deep learning using BiLSTM (Bidirectional Long Short Term Memory)\n","- A transformer-based algorithm using the Google BERT multilingual model \n","\n","In this notebook, you'll learn how to:\n","\n","- **Prepare your data** so that it can be used as training data for the Watson NLP classification algorithms.\n","- **Train a custom CRF model** using `watson_nlp.workflows.entity_mentions.SIRE`.\n","- **Train a BiLSTM** using `watson_nlp.blocks.entity_mentions.BiLSTM`.\n","- **Train a BERT** using `watson_nlp.workflows.entity_mentions.BERT`.\n","- **Store and load models** as an asset of a Watson Studio project.\n","\n","## Table of Contents\n","\n","1. [Before You Start](#beforeYouStart)\n","1.  [Prepare Training](#prepareTraining)\n","1.  [Model Building](#buildModel)\n","    1. [SIRE Training](#sire)\n","    1. [BiLSTM Training](#bilstm)\n","    1. [BERT Training](#bert)\n","1.  [Preparing Training Data](#Pre-Data)\n","1.  [Summary](#summary)"]},{"cell_type":"markdown","metadata":{},"source":["##### <a id=\"beforeYouStart\"></a>\n","## 1. Before You Start\n","\n","<div class=\"alert alert-block alert-danger\">\n","<b>Stop kernel of other notebooks.</b></div>\n","\n","**Note:** If you have other notebooks currently running with the _Default Python 3.x environment, **stop their kernels** before running this notebook. All these notebooks share the same runtime environment, and if they are running in parallel, you may encounter memory issues. To stop the kernel of another notebook, open that notebook, and select _File > Stop Kernel_.\n","\n","<div class=\"alert alert-block alert-warning\">\n","<b>Set Project token.</b></div>\n","\n","Before you can begin working on this notebook in Watson Studio in Cloud Pak for Data as a Service, you need to ensure that the project token is set so that you can access the project assets via the notebook.\n","\n","When this notebook is added to the project, a project access token should be inserted at the top of the notebook in a code cell. If you do not see the cell above, add the token to the notebook by clicking **More > Insert project token** from the notebook action bar.  By running the inserted hidden code cell, a project object is created that you can use to access project resources.\n","\n","![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n","\n","<div class=\"alert alert-block alert-info\">\n","<b>Tip:</b> Cell execution</div>\n","\n","Note that you can step through the notebook execution cell by cell, by selecting Shift-Enter. Or you can execute the entire notebook by selecting **Cell -> Run All** from the menu."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faker\n","  Downloading Faker-17.0.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from faker) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n","Installing collected packages: faker\n","Successfully installed faker-17.0.0\n"]}],"source":["!pip install faker"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["import json\n","import pandas as pd\n","import watson_nlp\n","from faker import Faker\n","import random \n","from watson_nlp import data_model as dm\n","from watson_nlp.toolkit.entity_mentions_utils import prepare_train_from_json, create_iob_labels"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Silence Tensorflow warnings\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR')\n","tf.autograph.set_verbosity(0)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["# Load a syntax model to split the text into sentences and tokens\n","syntax_model = watson_nlp.load(watson_nlp.download('syntax_izumo_en_stock'))"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"prepareTraining\"></a>\n","## 2. Preparing Training Data"]},{"cell_type":"markdown","metadata":{},"source":["The dataset is required to have a dictionary format as follows:\n","```\n","[\n","  {\n","    \"id\": 1,\n","    \"text\": \"This waterfall is actually hours away from Portland, basically in California.\",\n","    \"mentions\": \n","    [\n","      {\n","        \"text\": \"waterfall\", \"type\": \"GeographicFeature\", \n","        \"location\": \n","          {\n","            \"begin\": 5, \n","            \"end\": 14\n","          }\n","      },\n","      {\n","        \"text\": \"Portland\", \n","        \"type\": \"Location\", \n","        \"location\": \n","          {\n","            \"begin\": 43, \n","            \"end\": 51\n","          }\n","      },\n","      {\n","        \"text\": \"California\", \n","        \"type\": \"Location\", \n","        \"location\": \n","          {\n","            \"begin\": 66, \n","            \"end\": 76\n","          }\n","       }\n","    ]\n","  },\n","  ...\n","]\n","```"]},{"cell_type":"markdown","metadata":{},"source":["Since the data is already formatted correctly, the following process is needed to read the JSON data files from Watson Studio project assets and save them to the runtime working directory where they will be used as input for training the models."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["buffer = project.get_file(\"entity_train.json\")\n","pd.read_json(buffer).to_json('train.json', orient='records')\n","buffer = project.get_file(\"entity_dev.json\")\n","pd.read_json(buffer).to_json('dev.json', orient='records')\n","buffer = project.get_file(\"entity_test.json\")\n","pd.read_json(buffer).to_json('test.json', orient='records')"]},{"cell_type":"markdown","metadata":{},"source":["The text inputs will be converted into a streaming array where the text is broken down by the syntax model."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_data = dm.DataStream.from_json_array(\"train.json\")\n","train_iob_stream = prepare_train_from_json(train_data, syntax_model)\n","dev_data = dm.DataStream.from_json_array(\"dev.json\")\n","dev_iob_stream = prepare_train_from_json(dev_data, syntax_model)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"buildModel\"></a>\n","## 3. Model Building\n","\n","Entity extraction uses the entity-mentions block to encapsulate algorithms for the task of extracting mentions of entities (person, organizations, dates, locations,...) from the input text. The blocks and workflows offer implementations of strong entity extraction algorithms from each of the four families: rule-based, classic ML, deep-learning and transformers."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"sire\"></a>\n","### 3.1 SIRE Training"]},{"cell_type":"markdown","metadata":{},"source":["You can train SIRE models using either CRF & Maximum Entropy template as base models. Between the two, CRF based template takes longer to train but gives better results.\n","\n","These algorithms accept a set of featured in the form of dictionaries and regular expressions. A set of predefined feature extractors are provided for multiple languages, and you can also define your own features."]},{"cell_type":"code","execution_count":3,"metadata":{"scrolled":true},"outputs":[],"source":["#help(watson_nlp.workflows.entity_mentions.SIRE)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Download the algorithm template\n","mentions_train_template = watson_nlp.load(watson_nlp.download('file_path_entity-mentions_sire_multi_template-crf'))\n","# Download the feature extractor\n","default_feature_extractor = watson_nlp.load(watson_nlp.download('feature-extractor_rbr_entity-mentions_sire_en_stock'))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["{'log_code': '<NLP89404519W>', 'message': \"Dropping mention: Mention '3:25 p.m' (102, 110) overlaps with token 'p.m.' (107, 111) and has                                     been discarded. Ensure the entity span begins at the beginning of                                         a token and ends at the end of a token.\", 'args': None}\n","{'log_code': '<NLP89404519W>', 'message': \"Dropping mention: Mention 'St' (63, 65) overlaps with token 'St.' (63, 66) and has                                     been discarded. Ensure the entity span begins at the beginning of                                         a token and ends at the end of a token.\", 'args': None}\n","{'log_code': '<NLP35814863W>', 'message': 'Dropped 2 mentions in total from this text due to invalid mention spans', 'args': None}\n","{'log_code': '<NLP35814863W>', 'message': 'Dropped 2 mentions in total from this text due to invalid mention spans', 'args': None}\n","Initializing viterbi classifier\n","\u001b[32m[MEVitClassifier::initModel]\u001b[0m MEVitClassifier initialized.\n","\u001b[32m[MEVitClassifier2::initModel]\u001b[0m model initialized.\n","Get Feature str 17300\n","Done get feature str 17300\n","done. [25\u001b[33mg\u001b[0m559\u001b[33mm\u001b[0m580\u001b[33mk\u001b[0m,1\u001b[33mg\u001b[0m800\u001b[33mm\u001b[0m444\u001b[33mk\u001b[0m]\n","gramSize = 2\n","number of processes: 5\n","Initial processing:  (# of words: 1533, # of sentences: 129)\n","senIndex[1] = 16, wordIndex = 314\n","senIndex[2] = 36, wordIndex = 613\n","senIndex[3] = 67, wordIndex = 926\n","senIndex[4] = 97, wordIndex = 1226\n","senIndex[5] = 128, wordIndex = 1533\n","\u001b[32m[ME_CRF::scaleModel]\u001b[0m Updater -- l1=\u001b[32m0.1\u001b[0m, l2=\u001b[32m0.005\u001b[0m, history size=\u001b[32m5\u001b[0m, progress windows size \u001b[32m20\u001b[0m\n"," Iteration           Obj             WErr                         Timing       %Eff        Per thread timing\n","                 4892.48     14.02/ 61.24             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         0     2772.95     12.46/ 61.24             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         1      902.62      9.20/ 22.48             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         2      681.15      9.07/ 22.48             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         3      556.55      8.87/ 22.48             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.01]\n","         4      463.16      8.48/ 22.48             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         5      193.74      1.63/ 10.08             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","                  229.28      2.87/ 16.28             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         6      151.84      1.24/  6.20             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         7      130.35      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         8      122.85      0.00/  0.00             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","         9      108.59      0.00/  0.00             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        10       98.45      0.00/  0.00             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        11       90.11      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        12       83.09      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        13       79.67      0.00/  0.00             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        14       76.35      0.00/  0.00             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.01, av:0.01]\n","        15       74.34      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.01]\n","        16       72.10      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        17       70.58      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","        18       68.71      0.00/  0.00             E:0.02 s, M:0.00 s.       1.00 [m:0.02, M:0.02, av:0.02]\n","        19       67.69      0.00/  0.00             E:0.01 s, M:0.00 s.       1.00 [m:0.01, M:0.02, av:0.02]\n","Not enough progress in the last 5 iters.. converged.\n","    Thread     Total      Wait Effective      %Eff         #Sents/sec\n","         0      0.33      0.00      0.33      1.00             1571.48\n","         1      0.34      0.00      0.34      1.00             1524.30\n","         2      0.30      0.00      0.30      1.00             1827.83\n","         3      0.30      0.00      0.30      1.00             1978.52\n","         4      0.31      0.00      0.31      1.00             2128.61\n","Parent: the end!\n","Initializing viterbi classifier\n","\u001b[32m[MEVitClassifier::initModel]\u001b[0m MEVitClassifier initialized.\n","\u001b[32m[MEVitClassifier2::initModel]\u001b[0m model initialized.\n"]}],"source":["# Train the model\n","sire_custom = watson_nlp.workflows.entity_mentions.SIRE.train(syntax_model=syntax_model,\n","                                                              labeled_entity_mentions='/home/wsuser/work/', \n","                                                              #labeled_entity_mentions=train_data,\n","                                                              model_language='en', \n","                                                              template_resource=mentions_train_template, \n","                                                              feature_extractors=[default_feature_extractor], \n","                                                              l1=0.1, \n","                                                              l2=0.005, \n","                                                              num_epochs=50, \n","                                                              num_workers=5)"]},{"cell_type":"markdown","metadata":{},"source":["The following code will save the custom model to Watson Studio by using the project library."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Saved 4241 features.\n"]},{"data":{"text/plain":["{'file_name': 'sire_custom',\n"," 'message': 'File saved to project storage.',\n"," 'bucket_name': 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1',\n"," 'asset_id': '8b197850-caca-4071-ad73-b60d639c9f3c'}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Save the model\n","project.save_data('sire_custom', data=sire_custom.as_file_like_object(), overwrite=True)"]},{"cell_type":"markdown","metadata":{},"source":["Let's run the model on one example input from the dev dataset."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["'I work at California and Portland.'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["text = pd.read_json('dev.json')['text'][1]\n","text"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["{\n","  \"mentions\": [\n","    {\n","      \"span\": {\n","        \"begin\": 10,\n","        \"end\": 20,\n","        \"text\": \"California\"\n","      },\n","      \"type\": \"Duration\",\n","      \"producer_id\": null,\n","      \"confidence\": 0.9894799488701607,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    },\n","    {\n","      \"span\": {\n","        \"begin\": 25,\n","        \"end\": 33,\n","        \"text\": \"Portland\"\n","      },\n","      \"type\": \"Location\",\n","      \"producer_id\": null,\n","      \"confidence\": 0.9990983833955226,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    }\n","  ],\n","  \"producer_id\": {\n","    \"name\": \"Entity-Mentions SIRE Workflow\",\n","    \"version\": \"0.0.1\"\n","  }\n","}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Run the model\n","sire_result = sire_custom.run(text)\n","sire_result"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"bilstm\"></a>\n","### 3.2 BiLSTM Training"]},{"cell_type":"markdown","metadata":{},"source":["The deep-learning algorithm used in this block performs sequence labelling based on the BiLSTM architecture followed by a CRF layer. It uses GloVe embeddings as features."]},{"cell_type":"code","execution_count":4,"metadata":{"scrolled":true},"outputs":[],"source":["#help(watson_nlp.blocks.entity_mentions.BiLSTM)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# Download the GloVe model to be used as embeddings in the BiLSTM\n","glove_model = watson_nlp.load(watson_nlp.download('embedding_glove_en_stock'))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 9s 2s/step - loss: 1.4871 - val_loss: 1.3581\n","4/4 [==============================] - 0s 49ms/step - loss: 1.3068 - val_loss: 1.1875\n","4/4 [==============================] - 0s 49ms/step - loss: 1.1471 - val_loss: 1.0416\n","4/4 [==============================] - 0s 50ms/step - loss: 1.0084 - val_loss: 0.9220\n","4/4 [==============================] - 0s 48ms/step - loss: 0.9086 - val_loss: 0.8276\n","4/4 [==============================] - 0s 51ms/step - loss: 0.8241 - val_loss: 0.7550\n","4/4 [==============================] - 0s 48ms/step - loss: 0.7454 - val_loss: 0.6996\n","4/4 [==============================] - 0s 48ms/step - loss: 0.7035 - val_loss: 0.6567\n","4/4 [==============================] - 0s 50ms/step - loss: 0.6684 - val_loss: 0.6225\n","4/4 [==============================] - 0s 48ms/step - loss: 0.6241 - val_loss: 0.5941\n","4/4 [==============================] - 0s 48ms/step - loss: 0.5988 - val_loss: 0.5697\n","4/4 [==============================] - 0s 51ms/step - loss: 0.5715 - val_loss: 0.5480\n","4/4 [==============================] - 0s 50ms/step - loss: 0.5533 - val_loss: 0.5284\n","4/4 [==============================] - 0s 51ms/step - loss: 0.5268 - val_loss: 0.5105\n","4/4 [==============================] - 0s 49ms/step - loss: 0.5254 - val_loss: 0.4939\n","4/4 [==============================] - 0s 52ms/step - loss: 0.4995 - val_loss: 0.4785\n","4/4 [==============================] - 0s 51ms/step - loss: 0.4935 - val_loss: 0.4642\n","4/4 [==============================] - 0s 50ms/step - loss: 0.4732 - val_loss: 0.4507\n","4/4 [==============================] - 0s 50ms/step - loss: 0.4538 - val_loss: 0.4380\n","4/4 [==============================] - 0s 50ms/step - loss: 0.4585 - val_loss: 0.4259\n","4/4 [==============================] - 0s 49ms/step - loss: 0.4296 - val_loss: 0.4144\n","4/4 [==============================] - 0s 47ms/step - loss: 0.4382 - val_loss: 0.4034\n","4/4 [==============================] - 0s 51ms/step - loss: 0.4152 - val_loss: 0.3930\n","4/4 [==============================] - 0s 50ms/step - loss: 0.3984 - val_loss: 0.3831\n","4/4 [==============================] - 0s 50ms/step - loss: 0.3989 - val_loss: 0.3736\n","4/4 [==============================] - 0s 49ms/step - loss: 0.3777 - val_loss: 0.3646\n","4/4 [==============================] - 0s 48ms/step - loss: 0.3731 - val_loss: 0.3559\n","4/4 [==============================] - 0s 48ms/step - loss: 0.3673 - val_loss: 0.3476\n","4/4 [==============================] - 0s 56ms/step - loss: 0.3654 - val_loss: 0.3395\n","4/4 [==============================] - 0s 54ms/step - loss: 0.3575 - val_loss: 0.3318\n"]}],"source":["# Train the model\n","bilstm_custom = watson_nlp.blocks.entity_mentions.BiLSTM.train(train_iob_stream,\n","                                                              dev_iob_stream,\n","                                                              glove_model.embedding,\n","                                                              num_train_epochs=3)"]},{"cell_type":"markdown","metadata":{},"source":["The following code will save the custom model to Watson Studio by using the project library."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["{'file_name': 'bilstm_custom',\n"," 'message': 'File saved to project storage.',\n"," 'bucket_name': 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1',\n"," 'asset_id': 'a1420859-9a8e-4905-8448-e1a33bd6673a'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Save the model\n","project.save_data('bilstm_custom', data=bilstm_custom.as_file_like_object(), overwrite=True)"]},{"cell_type":"markdown","metadata":{},"source":["Let's run the model on one example input."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["{\n","  \"mentions\": [\n","    {\n","      \"span\": {\n","        \"begin\": 25,\n","        \"end\": 33,\n","        \"text\": \"Portland\"\n","      },\n","      \"type\": \"Location\",\n","      \"producer_id\": {\n","        \"name\": \"BiLSTM Entity Mentions\",\n","        \"version\": \"1.0.0\"\n","      },\n","      \"confidence\": 0.7481237649917603,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    }\n","  ],\n","  \"producer_id\": {\n","    \"name\": \"BiLSTM Entity Mentions\",\n","    \"version\": \"1.0.0\"\n","  }\n","}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Run the model\n","syntax_result = syntax_model.run(text)\n","bilstm_result = bilstm_custom.run(syntax_result)\n","bilstm_result"]},{"cell_type":"markdown","metadata":{},"source":["Now you are able to run the trained models on new data. You will run the models on the test data so that the results can also be used for model evaluation.\n","\n","Watson NLP includes methods for quality testing supported models. Given a model and test data, a quality report can be generated. The following example includes the steps required to generate a quality report for a BiLSTM entity mention extactor model. The same example can be applied to any entity mention extractor model."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING: Only Micro_avg metrics could be calculated based on the information available for this block type.\n"]},{"name":"stdout","output_type":"stream","text":["{\n","    \"per_class_confusion_matrix\": {\n","        \"Location\": {\n","            \"true_positive\": 1,\n","            \"false_positive\": 5,\n","            \"false_negative\": 12,\n","            \"precision\": 0.16666666666666666,\n","            \"recall\": 0.07692307692307693,\n","            \"f1\": 0.10526315789473684\n","        },\n","        \"GeographicFeature\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 2,\n","            \"false_negative\": 10,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Time\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 2,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Number\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 2,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Person\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 1,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Money\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 1,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Date\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 14,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Measure\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 8,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Duration\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 8,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Ordinal\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 3,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Percent\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 1,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Facility\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 4,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"JobTitle\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 3,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        },\n","        \"Organization\": {\n","            \"true_positive\": 0,\n","            \"false_positive\": 0,\n","            \"false_negative\": 3,\n","            \"precision\": 0.0,\n","            \"recall\": 0.0,\n","            \"f1\": 0.0\n","        }\n","    },\n","    \"macro_true_positive\": null,\n","    \"macro_false_positive\": null,\n","    \"macro_false_negative\": null,\n","    \"macro_precision\": 0.011904761904761904,\n","    \"macro_recall\": 0.005494505494505495,\n","    \"macro_f1\": 0.007518796992481203,\n","    \"micro_precision\": 0.125,\n","    \"micro_recall\": 0.0136986301369863,\n","    \"micro_f1\": 0.024691358024691357,\n","    \"overall_tp\": 1,\n","    \"overall_fp\": 7,\n","    \"overall_fn\": 72,\n","    \"detailed_metrics\": [],\n","    \"micro_precision_partial_match\": 0.0,\n","    \"micro_recall_partial_match\": 0.0,\n","    \"micro_f1_partial_match\": 0.0\n","}\n"]}],"source":["# Execute the model and generate the quality report\n","preprocess_func = lambda raw_doc: syntax_model.run(raw_doc)\n","quality_report = bilstm_custom.evaluate_quality('test.json', \n","                                               preprocess_func)\n","\n","# Print the quality report\n","print(json.dumps(quality_report, indent=4))"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"bert\"></a>\n","### 3.3 BERT Training"]},{"cell_type":"markdown","metadata":{},"source":["The algorithm used is a Transformer-based sequence labeling algorithm using the BERT architecture."]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true},"outputs":[],"source":["#help(watson_nlp.workflows.entity_mentions.BERT)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["{'log_code': '<NLP35814863W>', 'message': 'Dropped 2 mentions in total from this text due to invalid mention spans', 'args': None}\n","{'log_code': '<NLP96245348W>', 'message': 'Dropped 2 mentions in total from this text due to invalid mention spans', 'args': None}\n","{'log_code': '<NLP35814863W>', 'message': 'Dropped 2 mentions in total from this text due to invalid mention spans', 'args': None}\n","{'log_code': '<NLP96245348W>', 'message': 'Dropped 2 mentions in total from this text due to invalid mention spans', 'args': None}\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","3/3 [==============================] - 39s 9s/step - loss: 7826.1362 - test_accuracy: 0.3118 - val_loss: 2145.0308 - val_test_accuracy: 0.8984\n","Epoch 2/10\n","3/3 [==============================] - 23s 8s/step - loss: 2452.1787 - test_accuracy: 0.8111 - val_loss: 2215.0940 - val_test_accuracy: 0.9492\n","Epoch 3/10\n","3/3 [==============================] - 23s 8s/step - loss: 1668.1136 - test_accuracy: 0.9158 - val_loss: 600.1206 - val_test_accuracy: 0.9570\n","Epoch 4/10\n","3/3 [==============================] - 23s 9s/step - loss: 678.2223 - test_accuracy: 0.9438 - val_loss: 520.4328 - val_test_accuracy: 0.9531\n","Epoch 5/10\n","3/3 [==============================] - 22s 8s/step - loss: 478.7444 - test_accuracy: 0.9565 - val_loss: 316.7656 - val_test_accuracy: 0.9688\n","Epoch 6/10\n","3/3 [==============================] - 23s 8s/step - loss: 298.4914 - test_accuracy: 0.9730 - val_loss: 192.2734 - val_test_accuracy: 0.9805\n","Epoch 7/10\n","3/3 [==============================] - 23s 8s/step - loss: 181.1177 - test_accuracy: 0.9801 - val_loss: 122.0992 - val_test_accuracy: 0.9844\n","Epoch 8/10\n","3/3 [==============================] - 23s 9s/step - loss: 125.9435 - test_accuracy: 0.9840 - val_loss: 111.3857 - val_test_accuracy: 0.9844\n","Epoch 9/10\n","3/3 [==============================] - 23s 8s/step - loss: 114.2107 - test_accuracy: 0.9849 - val_loss: 109.2383 - val_test_accuracy: 0.9844\n","Epoch 10/10\n","3/3 [==============================] - 23s 8s/step - loss: 112.7157 - test_accuracy: 0.9852 - val_loss: 108.7209 - val_test_accuracy: 0.9844\n"]}],"source":["# Download and load the pretrained model resource\n","pretrained_model_resource = watson_nlp.load(watson_nlp.download('pretrained-model_bert_multi_bert_multi_cased'))\n","\n","# Labels you are interested in training the model for\n","labels = ['Duration', 'Location', 'GeographicFeature']\n","\n","# Generate IOB labels: B-Duration, I-Duration, B-Location, I-Location\n","iob_labels = create_iob_labels(labels)\n","\n","# Train the model\n","bert_custom = watson_nlp.workflows.entity_mentions.BERT.train(syntax_model_train_data_map={syntax_model:'train.json'}, \n","                                                              syntax_model_dev_data_map={syntax_model:'dev.json'},\n","                                                              label_list=labels,\n","                                                              pretrained_model_resource=pretrained_model_resource,\n","                                                              learning_rate=0.0005, \n","                                                              num_train_epochs=10, \n","                                                              do_lower_case=False, \n","                                                              train_max_seq_length=128, \n","                                                              train_stride=64, \n","                                                              train_batch_size=32, \n","                                                              dev_batch_size=32, \n","                                                              predict_batch_size=512, \n","                                                              predict_max_seq_length=48, \n","                                                              predict_stride=40, \n","                                                              keep_model_artifacts=False)"]},{"cell_type":"markdown","metadata":{},"source":["The following code will save the custom model to Watson Studio by using the project library."]},{"cell_type":"code","execution_count":21,"metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as word_embeddings_layer_call_fn, word_embeddings_layer_call_and_return_conditional_losses, embedding_postprocessor_layer_call_fn, embedding_postprocessor_layer_call_and_return_conditional_losses, encoder_layer_call_fn while saving (showing 5 of 324). These functions will not be directly callable after loading.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ec046ef20> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4e80d8e710> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ec046ecb0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ec046c7f0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ec046c790> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4e80f22410> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ec046c760> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ea811be50> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ea818b6a0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ea8133d90> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ea818aa10> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n","WARNING:absl:<watson_nlp.toolkit.bert_utils.model.Attention object at 0x7f4ea85373d0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class 'watson_nlp.toolkit.bert_utils.model.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]},{"data":{"text/plain":["{'file_name': 'bert_custom',\n"," 'message': 'File saved to project storage.',\n"," 'bucket_name': 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1',\n"," 'asset_id': '445cdcc2-9390-4f41-85c4-f5c11db2d86c'}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Save the model\n","project.save_data('bert_custom', data=bert_custom.as_file_like_object(), overwrite=True)"]},{"cell_type":"markdown","metadata":{},"source":["Let's run the model on one example input."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["{\n","  \"mentions\": [\n","    {\n","      \"span\": {\n","        \"begin\": 10,\n","        \"end\": 20,\n","        \"text\": \"California\"\n","      },\n","      \"type\": \"Duration\",\n","      \"producer_id\": {\n","        \"name\": \"BERT Entity Mentions\",\n","        \"version\": \"0.0.1\"\n","      },\n","      \"confidence\": 0.9898326992988586,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    },\n","    {\n","      \"span\": {\n","        \"begin\": 25,\n","        \"end\": 33,\n","        \"text\": \"Portland\"\n","      },\n","      \"type\": \"Location\",\n","      \"producer_id\": {\n","        \"name\": \"BERT Entity Mentions\",\n","        \"version\": \"0.0.1\"\n","      },\n","      \"confidence\": 0.9967482089996338,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    }\n","  ],\n","  \"producer_id\": {\n","    \"name\": \"BERT Entity Mentions Workflow\",\n","    \"version\": \"0.0.1\"\n","  }\n","}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Run the model\n","bert_result = bert_custom.run(text, 'en')\n","bert_result"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"Pre-Data\"></a>\n","## 4. Preparing Training Data"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["#Generate the dataset using faker\n","fake = Faker(locale='en_US')\n","\n","def format_data():\n","    # Generate a random degree level\n","    degree_level = fake.random_element(elements=('Bachelor\\'s', 'Master\\'s', 'Doctorate'))\n","\n","    # Generate a random field of study\n","    field_of_study = fake.random_element(elements=('Computer Science', 'Engineering', 'Business', 'Psychology','Medical'))\n","\n","    text_1 = \"I studied %s in %s\" %(degree_level,field_of_study)\n","    text_2 = \" Hello, I done my %s in %s\" %(degree_level, field_of_study)\n","    \n","    text = random.choice([text_1, text_2])\n","    \n","    \n","    field_of_study_begin = text.find(field_of_study)\n","    field_of_study_end = field_of_study_begin + len(field_of_study)\n","\n","    degree_level_begin = text.find(degree_level)\n","    degree_level_end = degree_level_begin + len(degree_level)\n","  \n","    \n","    \n","    data = {\n","                \"text\": text,\n","                \"mentions\": [\n","                    {\n","                        \"location\": {\n","                            \"begin\": field_of_study_begin,\n","                            \"end\": field_of_study_end\n","                        },\n","                        \"text\": field_of_study,\n","                        \"type\": \"field_of_study\"\n","                    },\n","                    {\n","                        \"location\": {\n","                            \"begin\": degree_level_begin,\n","                            \"end\": degree_level_end\n","                        },\n","                        \"text\": degree_level,\n","                        \"type\": \"degree_level\"\n","                    }\n","                ]   \n","            }\n","    \n","    return data"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["{'text': 'I studied Doctorate in Engineering',\n"," 'mentions': [{'location': {'begin': 23, 'end': 34},\n","   'text': 'Engineering',\n","   'type': 'field_of_study'},\n","  {'location': {'begin': 10, 'end': 19},\n","   'text': 'Doctorate',\n","   'type': 'degree_level'}]}"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["#Sample dataset\n","format_data()"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":["{'file_name': 'faker_Educational_text_train.json',\n"," 'message': 'File saved to project storage.',\n"," 'bucket_name': 'watsoncore-donotdelete-pr-olkxvfa8bk0pb1',\n"," 'asset_id': '263d99a1-954d-485f-ad9c-d81c95b5f526'}"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["#Prepared and store Training dataset for Driving License dataset\n","train_list_faker = []\n","for i in range(0, 10000):\n","    train_list_faker.append(format_data())\n","\n","with open('faker_Educational_text_train.json', 'w') as f:\n","    json.dump(train_list_faker, f)\n","project.save_data('faker_Educational_text_train.json', data=json.dumps(train_list_faker), overwrite=True)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["train_data = dm.DataStream.from_json_array(\"faker_Educational_text_train.json\")\n","train_iob_stream = prepare_train_from_json(train_data, syntax_model)\n","dev_data = dm.DataStream.from_json_array(\"faker_Educational_text_train.json\")\n","dev_iob_stream = prepare_train_from_json(dev_data, syntax_model)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["313/313 [==============================] - 15s 48ms/step - loss: 0.0929 - val_loss: 0.0039\n","313/313 [==============================] - 8s 26ms/step - loss: 0.0034 - val_loss: 0.0013\n","313/313 [==============================] - 8s 25ms/step - loss: 0.0015 - val_loss: 6.8544e-04\n","313/313 [==============================] - 8s 26ms/step - loss: 8.7299e-04 - val_loss: 4.1637e-04\n","313/313 [==============================] - 8s 25ms/step - loss: 5.7221e-04 - val_loss: 2.7751e-04\n","313/313 [==============================] - 8s 26ms/step - loss: 4.0044e-04 - val_loss: 1.9669e-04\n","313/313 [==============================] - 8s 26ms/step - loss: 2.9548e-04 - val_loss: 1.4502e-04\n","313/313 [==============================] - 8s 25ms/step - loss: 2.2406e-04 - val_loss: 1.1010e-04\n","313/313 [==============================] - 8s 25ms/step - loss: 1.7593e-04 - val_loss: 8.5297e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 1.3906e-04 - val_loss: 6.7257e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 1.1208e-04 - val_loss: 5.3689e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 9.1206e-05 - val_loss: 4.3296e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 7.5470e-05 - val_loss: 3.5148e-05\n","313/313 [==============================] - 8s 25ms/step - loss: 6.2198e-05 - val_loss: 2.8749e-05\n","313/313 [==============================] - 8s 25ms/step - loss: 5.1431e-05 - val_loss: 2.3681e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 4.3193e-05 - val_loss: 1.9568e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 3.6477e-05 - val_loss: 1.6228e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 3.0392e-05 - val_loss: 1.3510e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 2.5928e-05 - val_loss: 1.1240e-05\n","313/313 [==============================] - 8s 26ms/step - loss: 2.1809e-05 - val_loss: 9.4124e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 1.8525e-05 - val_loss: 7.8894e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 1.5887e-05 - val_loss: 6.5826e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 1.3327e-05 - val_loss: 5.5224e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 1.1332e-05 - val_loss: 4.6415e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 9.6677e-06 - val_loss: 3.8958e-06\n","313/313 [==============================] - 8s 27ms/step - loss: 8.1768e-06 - val_loss: 3.2901e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 7.1083e-06 - val_loss: 2.7573e-06\n","313/313 [==============================] - 8s 25ms/step - loss: 6.0920e-06 - val_loss: 2.3238e-06\n","313/313 [==============================] - 8s 26ms/step - loss: 5.1533e-06 - val_loss: 1.9570e-06\n","313/313 [==============================] - 8s 25ms/step - loss: 4.4889e-06 - val_loss: 1.6604e-06\n"]}],"source":["# Train BILSTM Model for Educational details entity\n","bilstm_custom = watson_nlp.blocks.entity_mentions.BiLSTM.train(train_iob_stream,\n","                                                              dev_iob_stream,\n","                                                              glove_model.embedding,\n","                                                              num_train_epochs=5)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["text= \"Hello, I done my master of Engineering\""]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"data":{"text/plain":["{\n","  \"mentions\": [\n","    {\n","      \"span\": {\n","        \"begin\": 17,\n","        \"end\": 23,\n","        \"text\": \"master\"\n","      },\n","      \"type\": \"degree_level\",\n","      \"producer_id\": {\n","        \"name\": \"BiLSTM Entity Mentions\",\n","        \"version\": \"1.0.0\"\n","      },\n","      \"confidence\": 0.9999746084213257,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    },\n","    {\n","      \"span\": {\n","        \"begin\": 27,\n","        \"end\": 38,\n","        \"text\": \"Engineering\"\n","      },\n","      \"type\": \"field_of_study\",\n","      \"producer_id\": {\n","        \"name\": \"BiLSTM Entity Mentions\",\n","        \"version\": \"1.0.0\"\n","      },\n","      \"confidence\": 0.9999901056289673,\n","      \"mention_type\": \"MENTT_UNSET\",\n","      \"mention_class\": \"MENTC_UNSET\",\n","      \"role\": \"\"\n","    }\n","  ],\n","  \"producer_id\": {\n","    \"name\": \"BiLSTM Entity Mentions\",\n","    \"version\": \"1.0.0\"\n","  }\n","}"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["# Run the BILSTM model\n","syntax_result = syntax_model.run(text)\n","bilstm_result = bilstm_custom.run(syntax_result)\n","\n","bilstm_result"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"summary\"></a>\n","## 5. Summary"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"color:blue\">This notebook shows you how to use the Watson NLP library and how quickly and easily you can train and run different entity extraction models using Watson NLP.</span>"]},{"cell_type":"markdown","metadata":{},"source":["Please note that this content is made available to foster Embedded AI technology adoption. The content may include systems & methods pending patent with USPTO and protected under US Patent Laws. For redistribution of this content, IBM will use release process. For any questions please log an issue in the [GitHub](https://github.com/ibm-build-labs/Watson-NLP). \n","\n","Developed by IBM Build Lab \n","\n","Copyright - 2022 IBM Corporation "]}],"metadata":{"kernelspec":{"display_name":"Python 3.10 + GPU","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":1}
